altair>=5.5.0
attrs>=25.4.0
blinker>=1.9.0
cachetools>=6.2.4
certifi>=2026.1.4
cffi>=2.0.0
charset-normalizer>=3.4.4
click>=8.1.8
contourpy>=1.3.0
cryptography>=46.0.3
cycler>=0.12.1
et_xmlfile>=2.0.0
fonttools>=4.60.2
gitdb>=4.0.12
GitPython>=3.1.46
idna>=3.11
importlib_resources>=6.5.2
Jinja2>=3.1.6
jsonschema>=4.25.1
jsonschema-specifications>=2025.9.1
kiwisolver>=1.4.7
MarkupSafe>=3.0.3
matplotlib>=3.9.4
narwhals>=2.15.0
numpy>=2.0.2
openpyxl>=3.1.5
packaging>=25.0
pandas>=2.3.3
pillow>=11.3.0
protobuf>=6.33.4
pyarrow>=21.0.0
pycparser>=2.23
pydeck>=0.9.1
pyparsing>=3.3.1
python-dateutil>=2.9.0.post0
pytz>=2025.2
referencing>=0.36.2
reportlab>=4.4.9
requests>=2.32.5
rpds-py>=0.27.1
scipy>=1.13.1
seaborn>=0.13.2
six>=1.17.0
smmap>=5.0.2
streamlit>=1.50.0
tenacity>=9.1.2
toml>=0.10.2
tornado>=6.5.4
typing_extensions>=4.15.0
tzdata>=2025.3
urllib3>=2.6.3
watchdog>=6.0.0
zipp>=3.23.0

# AI Chat dependencies (Gemma 3 4B support)
llama-cpp-python>=0.3.0
huggingface-hub>=0.20.0
psutil>=6.0.0

# Optional GPU acceleration (install separately if needed)
# For Metal (Apple Silicon): pip install --pre --upgrade llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/metal
# For CUDA: CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --no-cache-dir
